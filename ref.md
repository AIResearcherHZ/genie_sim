
Genie Sim Benchmark

    User Guide
        1. Overview
        2. Quick Start
            2.1 Requirements
            2.2 Download Scenes & Assets from Huggingface
            2.3 Installation
                2.3.1 Docker Container (recommended)
                2.3.2 Host Machine
                2.3.3 Developer Guide
                2.3.4 Enable pre-commit hooks (optional)
            2.4 Benchmark Tasks
                2.4.1 Run Benchmark
                2.4.2 Benchmark Evaluation Metrics
            2.5 Teleoperation
                2.5.1 PICO
                2.5.2 Keyboard
            2.6 Record & Replay
                2.6.1 Replay
        3. Use Cases
            3.1 How to set up a benchmark task file
            3.2 How to create a complete benchmark task with embodied AI model
            3.3 Best Practice of Asset Parameters for Manipulation Task
                3.3.1 How to debug collider in IsaacSim
                3.3.2 Bad Examples
                3.3.3 Good Examples
        4. API Reference

User Guide

Genie Sim
1. Overview

Genie Sim is the simulation framework from AgiBot, which provides developers efficient data generation capabilities and evaluation benchmarks to accelerate embodied intelligence development. Genie Sim has established a comprehensive closed loop pipeline, encompassing trajectory generation, model training, benchmarking, and deployment validation. Users can quickly validate algorithm performance and optimize models through this efficient simulation toolchain. Whether for simple grasping tasks or complex long-range operations, Genie Sim can provide a highly realistic simulation environment and precise evaluation metrics, empowering developers to efficiently complete the development and iteration of robotic technologies.

Genie Sim Benchmark, as the open-source evaluation version of Genie Sim, is dedicated to providing precise performance testing and optimization support for embodied AI models.
2. Quick Start
2.1 Requirements
Minimum 	Tested

    Ubuntu 22.04
    NVIDIA Isaac Sim 4.5.0
    Hardware
        CPU: Inter Core i7(7th Generation)/AMD Ryzen 5 or higher
        RAM: 32GB
        GPU: GeForce RTX 3070 or higher
        Driver: 535.129.03+
        Storage: 50GB SSD or larger

	

    Ubuntu 22.04
    NVIDIA Isaac Sim 4.5.0
    Hardware
        CPU: Inter Core i7(12th Generation)
        RAM: 64GB
        GPU: GeForce RTX 4090D
        Driver: 550.120 + CUDA 12.4
        Storage: 1TB NVMe SSD

    https://docs.isaacsim.omniverse.nvidia.com/latest/installation/requirements.html

2.2 Download Scenes & Assets from Huggingface

Please visit https://huggingface.co/datasets/agibot-world/GenieSimAssets and follow the instructions
2.3 Installation
2.3.1 Docker Container (recommended)

    Develop using docker container
        Install docker following Isaac Sim Documentation
    Prepare docker image

# cd in genie_sim root dir and create docker image from dockerfile
docker build -f ./scripts/dockerfile -t registry.agibot.com/genie-sim/open_source:latest .

    Run docker container and start server

# start a new container in main direcory
# you need to change ~/assets to GenieSimAssets folder
SIM_ASSETS=~/assets ./scripts/start_gui.sh
./scripts/into.sh

# start server
omni_python server/source/genie.sim.lab/raise_standalone_sim.py --enable_curobo True # The grasp trajectory of this demo is generated by Curobo

    Run a supermarket demo in a new container

# start container in main directory
./scripts/into.sh

# start client in container
omni_python benchmark/task_benchmark.py --task_name=genie_task_supermarket

2.3.2 Host Machine

    We recommend developer using our unified docker container environment
    If you wish to use your own environment, please refer to dockerfile we provide and install the dependencies that we list

2.3.3 Developer Guide
2.3.4 Enable pre-commit hooks (optional)

    Install and setup pre-commit to enable auto file-formatter, python / json / yaml etc.

# install pre-commit to your python env
sudo apt install python3-pip
pip3 install pre-commit

# enable pre-defined pre-commit-hooks within repo
pre-commit install

    Trigger file-formatter for all tracked files

pre-commit run --all-files

2.4 Benchmark Tasks
2.4.1 Run Benchmark

    Run docker container and start server

# start a new container in main direcory
# you need to change ~/assets to GenieSimAssets folder
SIM_ASSETS=~/assets ./scripts/start_gui.sh
./scripts/into.sh

# start server in container
omni_python server/source/genie.sim.lab/raise_standalone_sim.py

    Run benchmark in docker in a new container

# start container in main directory
./scripts/into.sh

# start client in container
omni_python benchmark/task_benchmark.py --task_name=genie_task_supermarket

    You can run other benchmarks by using different "task_name". Current provided benchmark scenarios are listed below and can be found in the directory "/benchmark/bdd/eval_tasks"
        genie_task_cafe_espresso
        genie_task_cafe_toast
        genie_task_home_clean_desktop
        genie_task_home_collect_toy
        genie_task_microwave_food
        genie_task_open_drawer
        genie_task_pack_fruit
        genie_task_pass_water
        genie_task_pour_water
        genie_task_supermarket
        genie_task_supermarket_cashier_packing
        genie_task_wipe_dirt

2.4.2 Benchmark Evaluation Metrics

Configuration
Use BDDL (Behavior Domain Definition Language) for evaluation configuration
Example

(define (problem genie_place_trash_into_dustbin_0)
    (:domain isaacsim)

    (:objects
        genie_napkin_001 - napkin.n.01
        genie_dustbin_009 - dustbin.n.01
    )

    (:init
        (onfloor genie_napkin_001 floor.n.01_2)
    )

    (:goal
        (inside ?genie_napkin_001 ?genie_dustbin_009)
    )
)

Current Evaluation Capability
Checker 	Description 	Keywords
OnTop 	Is the object above the certain object 	ontop
Inside 	Do the two objects contain 	inside
OnShelf 	Is the object in a certain area 	onshelf
Open 	Has the object been opened 	open
Pass2People 	Is the object pointing towards a person in the scene 	pass2people

Evaluation Output Data Structure

# output/
[
    {
        "task_type": "benchmark",
        "model_path": "/data/checkpoint-53400",
        "task_uid": "09d714f6-b0ad-41b0-8826-03a432dbf37f",
        "task_name": "genie_pass_water_to_people",
        "stage": "",
        "result": {
            "code": -1,
            "step": 0,
            "msg": ""
        },
        "start_time": "2025-03-13 15:33:01",
        "end_time": "2025-03-13 15:34:32"
    }
]

Error Code

from enum import Enum

class ErrorCode(Enum):
    INIT_VALUE = -1
    SUCCESS = 0
    ABNORMAL_INTERRUPTION = 1
    OUT_OF_MAX_STEP = 2

    UNKNOWN_ERROR = 500

2.5 Teleoperation
2.5.1 PICO

Support joystick control for robot waist, head, left/right end effector and base movement

User Guide
pico
No. 	Function (Left / Right)
① 	spin: move base of the robot
press: reset base pose 	spin: control robot's waist
press + spin: control robot's head
② 	reset left arm
③ 	enable pose tracking
④ 	reset right arm
⑤ 	reset body and head
⑥ 	control left gripper 	control right gripper

Pico Setup

    Connect to the same LAN as the computer
    Start AIDEA Vision App in resource library
    Choose Wireless Connection and enter the IP of the computer

Launch Setup

    Start server

    # run this in the container
    omni_python server/source/genie.sim.lab/raise_standalone_sim.py

    Start PICO control in the container

    # run this in the container
    omni_python teleop/teleop.py --task_name genie_task_xxx --mode pico --host_ip x.x.x.x

2.5.2 Keyboard

Support keyboard control for robot waist, head, left/right end effector and base movement
User Guide
Key 	Function 	Key 	Function
w 	eef forward 	i 	roll +
s 	eef backward 	k 	roll -
a 	eef left 	j 	pitch +
d 	eef right 	l 	pitch -
q 	eef up 	u 	yaw +
e 	eef down 	o 	yaw -
up 	base forward 	shift+up 	head pitch +
down 	base backward 	shift+down 	head pitch -
left 	base turn left 	shift+left 	head yaw +
right 	base turn right 	shift+right 	head yaw -
ctrl+up 	waist up 	ctrl+tab 	switch arm
ctrl+down 	waist down 	r 	reset
ctrl+left 	waist pitch - 	c 	close gripper
ctrl+right 	waist pitch + 	ctrl+c 	open gripper

Launch Setup

    Start server

    # run this in the container
    omni_python server/source/genie.sim.lab/raise_standalone_sim.py

    Start keyboard control in the container

    # run this in the container
    omni_python teleop/teleop.py --task_name genie_task_xxx --mode "keyboard" 

2.6 Record & Replay

For efficiency reason, trajectory is first recorded and then replay is executed to record videos.
Record all scenario information in state.json file, including robot joint positions, object pose, camera pose etc.

    Start server

    # run this in the container
    omni_python server/source/genie.sim.lab/raise_standalone_sim.py --rospub   

    Start client

    # run this in the container
    omni_python teleop/teleop.py --task_name genie_task_xxx --mode "keyboard"  --record 

The scenario info is recorded at ./output/recording_data/{genie_task_xxx}/state.json
2.6.1 Replay

Replay trajectory and record videos

    Start server

    # run this in the container
    omni_python server/source/genie.sim.lab/raise_standalone_sim.py --rospub --record_img --disable_physics

    Start client

    # run this in the container
    omni_python teleop/replay_state.py --task_file teleop/tasks/genie_task_xxx.json --state_file output/recording_data/genie_task_xxx/state.json --record

The video is recorded at ./output/recording_data/{genie_task_xxx}/{idx}/
3. Use Cases
3.1 How to set up a benchmark task file

Each benchmark task file is structured as below with 6 key elements:

    The task contains a unique task name
    The objects contain a variety of task objects:
        extra_objects: none-interactive objects
        fix_objects: interactive objects with fixed init pose
        task_related_objects: interactive objects with random init pose
    The recording_setting specifies the camera views to be recorded
    The robot includes the robot id, robot config file and robot base init pose
    The scene specifies a set of scene information:
        scene_id: a unique name of the scene
        function_space_objects: a cubic area where task_related_objects are randomly generated
        scene_info_dir: the path of the scene assets
        scene_usd: the path of the scene usd file
    The stages contain several substages for planned tasks
        action: define action name
        active: define active object, such as gripper
        passive: define passive object, such as bottle

3.2 How to create a complete benchmark task with embodied AI model

    First, assemble your task template like "genie_task_supermarket.json". This configuration file contains robot, scene, objects and stages etc. 

benchmark/bddl/eval_tasks/your_task.json

Please fill in the assets file path correctly. The program will look for the assets in the user-configured environment variable SIM_ASSETS.

    Scene generalization: mapping your task and scenes

File path: benchmark/bddl/task_to_preselected_scenes.json

{
  "genie_task_supermarket": [
    "scenes/genie/supermarket_02/Collected_emptyScene_01/emptyScene_01.usd"
  ],
}

    Code evaluation standards, for example

File Path: benchmark/bddl/task_definitions/genie_task_supermarket/problem0.bddl

(define (problem restock_shelves)
    (:domain isaac)

    (:objects
        benchmark_beverage_bottle_013 - bottle.n.01
    )

    (:init
        (onfloor benchmark_beverage_bottle_013 floor.n.01_2)
    )

    (:goal
        (onshelf ?benchmark_beverage_bottle_013)
    )
)

Now, benchmark configuration step is complete.

    Access your embodied intelligence model under "yourpolicy.py". (Please refer to demopolicy.py)

class YourPolicy(BasePolicy):
    def __init__(self) -> None:
        super().__init__()
        """Init config and load model."""
        pass

    def reset(self):
        """Reset."""
        pass

    def act(self, observations, **kwargs) -> np.ndarray:
        """Act based on the observations.
        Args: observations contain robot image(Head_Camera_01/Right_Camera_01/Left_Camera_01) and current joint information
        Returns: Robot target_joint
        """
        pass

    Finally, run client and evaluate your own AI model

python3 benchmark/task_benchmark.py --task_name {task name} --policy_class {policy name} --env_class OmniEnv

3.3 Best Practice of Asset Parameters for Manipulation Task
3.3.1 How to debug collider in IsaacSim

isaacsim

    Follow the instructions to enable collider debug mode

3.3.2 Bad Examples

    Complicated mesh leads to collision computation error, simpler the better
    Too many tri-mesh in a small area usually leads to mesh collision

图片alt
convexHull and convexDecompisition
图片alt
convexDecompisition
图片alt
convexDecompisition
图片alt
convexDecompisition
图片alt
convexDecompisition
图片alt
convexDecompisition
图片alt
convexDecompisition
3.3.3 Good Examples

Good means easy to grasp and less mesh collsion

    Good collision mesh usually looks simple and neat
    Make sure the key area for grasping is just perfect(eg. Tincan, bottle, bottle cap)

图片alt
convexDecompisition Mesh
图片alt
convexDecompisition Collider
图片alt
convexHull
图片alt
convexHull
图片alt
convexHull
图片alt
convexHull
4. API Reference
	Function 	Description 	Input 	Return
Initialize 	client.InitRobot() 	Initialize robot and scene 	robot_cfg: config file paths (located in 'robot_cfg' folder)
scene_usd: usd path
init_position([x,y,z])
init_rotation([x,y,z]) 	
Controller 	client.moveto() 	Robot's end effector moves to target pose 	target_position([x,y,z])
target_quaternion([w,x,y,z])
is_backend:

    True: ee straight move
    False: ee move with Collision Avoidence

ee_interpolation:

    True: ruckig interpolation
    False: target point interpolation

distance_frame: interpolation density
	
client.set_joint_positions() 	Robot's joint moves to target joint state 	target_joint_position: [N]， rad/s
joint_indices: [N], index
is_trajectory:

    True: joint apply action
    False: set joint state

	
client.set_gripper_state() 	Set gripper state(open or close) 	gripper_command: "open" or "close"
is_right(bool)
opened_width(float)
	
client.SetTrajectoryList() 	Set ee pose list trajectory 	trajectory_list(list): [[position(xyz), rotation(wxyz)]]
	
Object 	client.add_object() 	Add an USD object into scene 	usd_path(str): asset's relative path based on SIM_ASSETS
prim_path(str): object's prim path in scene exam e.g: "/World/Object/obj_01"
lable_name(str): object's label name which applies for semantic segmentation
target_position([x,y,z])
target_quaternion([w,x,y,z])
target_scale([x,y,z])
mass:[kg]
add_particle(bool):add additional fluid particle e.g: water in kettle
particle_size:([x,y,z])
particle_position:([x,y,z])
	
client.SetMaterial() 	Set material of XFormPrim 	material_info(list):
e.g:
material_info = [
 {"object_prim" : "/obj1"
 "material_name": "wood"
 "material_path: "materials/wood"}] 	
client.SetLight() 	Set Light info of stage 	light_info(list):
e.g:
light_info = [{
 "light_type": "Distant",
 "light_prim": "/World/DistantLight",
 "light_temperature": 2000,
 "light_intensity": 1000,
 "rotation": [1,0.5,0.5,0.5],
 "texture": ""}] 	
client.SetObjectPose() 	Set rigid and articulated object pose in single physics step 	object_info(list):
e.g:
object_info = [{
 "prim_path": "/World/obj1",
 "position": [x,y,z],
 "rotation": [w,x,y,z]}]
object_joint_info(list): Articulated object joint info 	
Sensor 	client.AddCamera() 	add a camera into stage 	camera_prim(str): camera's prim path
camera_position(xyz): camera's position
camera_rotation(wxyz): camera's rotation
Width, Height, focus_length, horizontal_aperture
vertical_aperture camera's intrinsic parameters:
 fx = Width *focus_length / horizontal_aperture
 fy = Height * focus_length / vertical_aperture
is_local(bool): local pose or world pose 	
client.capture_frame() 	Capture rgb/depth frame 	camera_prim(str): camera's prim path 	response.color_image.data
response.depth_image.data
client.capture_semantic_frame() 	Capture semantic frame 	camera_prim_path(str): camera's prim path 	response.semantic_mask.data
Observation 	client.get_observation() 	Get camera/joint/tf data in single frame 	data_keys(dict):
e.g:
data_keys = { 'camera': {
  'camera_prim_list': [“/camera”],
  'render_depth': True,
  'render_semantic': True
  },
  'pose': ["/object1" ],
  'joint_position': True,
  'gripper': True} 	observation = {
 "camera": camera_datas,
 "joint": joint_datas,
 "pose": object_datas,
 "gripper": gripper_datas }
client.GetIKStatus() 	Inverse kinematic calculation 	target_poses(list): [{"position":xyz, "rotation":wxyz}]
is_right(bool): arm type of dual arms
ObsAvoid(bool): calculate IK with collision avoidence
	"status"([bool]): Inverse kinematic success,
"Jacobian"([double]): Jacobian score of ik joint state,
"joint_positions"([list]):ik joint positions
"joint_names"([list]):ik joint names
client.GetEEPose() 	Get end effector's world pose 	is_right(bool): choose arm type
	state.ee_pose.position
state.ee_pose.rpy
client.get_object_pose() 	Get object's world pose 	prim_path(str): object prim path
	object_pose.position
object_pose.rpy
client.get_joint_positions() 	Get robot's current joint positions 		result.states.name
result.states.position
Curobo featured function 	client.AttachObj() 	Attach passive object 	prim_paths[list[str]]: attached object prim paths 	
client.DetachObj() 	Detach all objects 		
recording settings 	client.start_recording() 	recording episode 	data_keys(dict): recording settings
e.g:
data_keys = {
 'camera': {
  'camera_prim_list': [
   '/World/base_link/Head_Camera'
  ],
  'render_depth': True,
  'render_semantic': True
 },
 'pose': [
  '/World/obj1'
 ],
 'joint_position': True,
 'gripper': True
}
fps(int): recording fps task_name(str) 	
client.stop_recording() 	stop recording 		
client.reset() 	reset scene、robot and object 		
client.Exit() 	exit application 		
